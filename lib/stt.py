import os.path
import time
import wave
import vosk
import json
import pyaudio
import numpy as np
import speech_recognition as sr
import config.config


class AdvancedVAD:
    def __init__(self, rate=16000, chunk=1024):
        self.rate = rate
        self.chunk = chunk
        self.audio = pyaudio.PyAudio()

        # VADå‚æ•°
        self.energy_threshold = 300  # èƒ½é‡é˜ˆå€¼
        self.silence_limit = 3  # é™éŸ³é™åˆ¶ï¼ˆç§’ï¼‰
        self.previous_energy = 0
        self.energy_delta = 100  # èƒ½é‡å˜åŒ–é˜ˆå€¼

        # çŠ¶æ€
        self.is_recording = False
        self.recording_started = False

    def calculate_energy(self, data):
        """è®¡ç®—éŸ³é¢‘èƒ½é‡"""
        audio_data = np.frombuffer(data, dtype=np.int16)
        if len(audio_data) == 0:
            return 0
        return np.mean(np.abs(audio_data))

    def is_speech(self, data):
        """æ£€æµ‹æ˜¯å¦ä¸ºè¯­éŸ³"""
        energy = self.calculate_energy(data)

        # åŸºäºèƒ½é‡å’Œèƒ½é‡å˜åŒ–çš„è¯­éŸ³æ£€æµ‹
        is_above_threshold = energy > self.energy_threshold
        has_energy_change = abs(energy - self.previous_energy) > self.energy_delta

        self.previous_energy = energy

        return is_above_threshold or has_energy_change

    def record_until_silence(self, callback=None, max_duration=10):
        """å½•åˆ¶ç›´åˆ°æ£€æµ‹åˆ°3ç§’é™éŸ³"""
        self.is_recording = True
        self.recording_started = False

        audio_buffer = []
        silence_frames = 0
        speech_frames = 0

        stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )

        print("ğŸ¤ ç­‰å¾…è¯­éŸ³å¼€å§‹...")

        try:
            while self.is_recording:
                data = stream.read(self.chunk, exception_on_overflow=False)

                if self.is_speech(data):
                    if not self.recording_started:
                        print("ğŸ—£ï¸ æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå¼€å§‹å½•åˆ¶...")
                        self.recording_started = True

                    audio_buffer.append(data)
                    silence_frames = 0
                    speech_frames += 1

                else:
                    if self.recording_started:
                        silence_frames += 1
                        audio_buffer.append(data)  # ä»ç„¶ä¿å­˜é™éŸ³å¸§

                # æ£€æŸ¥åœæ­¢æ¡ä»¶
                silence_seconds = silence_frames * self.chunk / self.rate
                total_seconds = len(audio_buffer) * self.chunk / self.rate

                # å¦‚æœå·²ç»å¼€å§‹å½•åˆ¶ä¸”æ£€æµ‹åˆ°3ç§’é™éŸ³ï¼Œåœæ­¢
                if self.recording_started and silence_seconds >= self.silence_limit:
                    print(f"æ£€æµ‹åˆ°{silence_seconds:.1f}ç§’é™éŸ³ï¼Œåœæ­¢å½•åˆ¶")
                    break

                # æœ€å¤§å½•åˆ¶æ—¶é•¿ä¿æŠ¤
                if total_seconds >= max_duration:
                    print("è¾¾åˆ°æœ€å¤§å½•åˆ¶æ—¶é•¿ï¼Œåœæ­¢å½•åˆ¶")
                    break

        except Exception as e:
            print(f"å½•åˆ¶é”™è¯¯: {e}")

        finally:
            stream.stop_stream()
            stream.close()

            filename = os.path.join(config.config.TTS_TO_SPEECH_TMP_DIR, f"command_{int(time.time())}.wav")
            if audio_buffer and self.recording_started:
                self.save_audio(audio_buffer, filename)
                print(f"âœ… å½•åˆ¶å®Œæˆ: {filename}")

                if callback:
                    callback(filename)
            else:
                print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³")
            return filename

    def save_audio(self, frames, filename):
        """ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
        import wave

        wf = wave.open(filename, 'wb')
        wf.setnchannels(1)
        wf.setsampwidth(self.audio.get_sample_size(pyaudio.paInt16))
        wf.setframerate(self.rate)
        wf.writeframes(b''.join(frames))
        wf.close()


class SpeechToText:
    """è¯­éŸ³è½¬æ–‡æœ¬ç±»"""

    @staticmethod
    def speech_to_text(audio_file):
        """ä½¿ç”¨SpeechRecognitionè¿›è¡Œè¯­éŸ³è¯†åˆ«"""
        try:
            r = sr.Recognizer()
            with sr.AudioFile(audio_file) as source:
                # è°ƒæ•´ç¯å¢ƒå™ªå£°
                r.adjust_for_ambient_noise(source, duration=0.5)
                audio = r.record(source)

            # ä½¿ç”¨Googleè¯­éŸ³è¯†åˆ«
            text = r.recognize_google(audio, language='zh-CN')
            return text

        except ImportError:
            print("è¯·å®‰è£…SpeechRecognition: pip install SpeechRecognition")
            return ""
        except Exception as e:
            print(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            return ""

    @staticmethod
    def speech_to_text_by_vosk(audio_file, model_path=config.config.VOSK_MODEL):
        """ä½¿ç”¨Voskç¦»çº¿è¯†åˆ«"""
        model = vosk.Model(model_path)
        wf = wave.open(audio_file, "rb")

        recognizer = vosk.KaldiRecognizer(model, wf.getframerate())

        results = []
        while True:
            data = wf.readframes(4000)
            if len(data) == 0:
                break
            if recognizer.AcceptWaveform(data):
                result = json.loads(recognizer.Result())
                results.append(result.get('text', ''))

        final_result = json.loads(recognizer.FinalResult())
        results.append(final_result.get('text', ''))

        return ' '.join(results)


# é›†æˆåˆ°çƒ­è¯ç³»ç»Ÿ
def integrate_with_hotword():
    """é›†æˆåˆ°çƒ­è¯æ£€æµ‹ç³»ç»Ÿ"""
    vad_system = AdvancedVAD()
    stt = SpeechToText()

    def hotword_callback():
        """çƒ­è¯æ£€æµ‹å›è°ƒ"""
        print("ğŸ”¥ çƒ­è¯å”¤é†’ï¼")
        vad_system.record_until_silence(callback=process_command)

    def process_command(audio_file):
        """å¤„ç†æŒ‡ä»¤"""
        command_text = stt.speech_to_text(audio_file)
        if command_text:
            print(f"ğŸ¯ æŒ‡ä»¤: {command_text}")
            # æ‰§è¡Œç›¸åº”æ“ä½œ

    return hotword_callback


# if __name__ == "__main__":
#     vad_system = AdvancedVAD()
#     stt = SpeechToText()
#     audio_file = vad_system.record_until_silence()
#     print("audio_file:", audio_file)
#     ret = stt.speech_to_text_by_vosk(audio_file)
#     print("ret:", ret)